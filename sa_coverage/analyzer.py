from json import dumps
from .loading import LoadingIcon
from os import path, remove
from re import Match, match
from subprocess import check_output, STDOUT
from typing import Dict, List, Optional, Set
from xml.etree import ElementTree as ET

DELIMITER: str = "|%$%|"
NUM_ERROR_LINES: int = 2
PATTERN_FILE_PATH: str = r"(.*?\.[\w:]+)"
PATTERN_ERROR: str = fr"^{PATTERN_FILE_PATH}:\d+:\d+: error:"
PATTERN_FILE_NAME_NO_CONFIG: str = fr"^Checking {PATTERN_FILE_PATH} ...$"
PATTERN_FILE_NAME_WITH_CONFIG: str = fr"^Checking {PATTERN_FILE_PATH}: \w+...$"
PATTERN_NUM_CONFIGS: str = r"^~~~ NUMBER OF PREPROCESSOR CONFIGS: \d+ ~~~$"
PATTERN_FILE_TOKENS: str = r"^~~~ FILE TOKENS: ~~~$"
PATTERN_NUM_CHECKS: str = r"^~~~ Ran \d+ checks for this above chunk ~~~$"
EXT_CTU_INFO: str = ".ctu-info"
EXT_DUMP: str = ".dump"

def analyze(path_to_analyzer: str, cmd_args: str, *, fname: str) -> None:
    """
    Accepts command line arguments to be passed to a static analyzer
    as if we were just running that analyzer.

    Parameters:
        path_to_analyzer (str): The analyzer in question.
        cmd_args (str): The arguments to be passed to the analyzer.
        fname (str): The resulting file to dump the parsed info to.

    Returns:
        None.
    """
    output_raw: Optional[bytes] = None
    stdout: str = ""
    parsed: Dict[str, Dict[str, Dict[str, int|str|list ]]] = {}
    
    with LoadingIcon(f"Running {path_to_analyzer} {cmd_args}..."):
        output_raw = check_output(f"{path_to_analyzer} {cmd_args} --dump", stderr=STDOUT, shell=True)

    with LoadingIcon("Parsing output..."):
        stdout = output_raw.decode()
        parsed = _parse_stdout(stdout)

    for file_path, config_dict in parsed.items():
        for config_name, info in config_dict.items():
            if info.get("toks") is None:
                print(file_path, config_name)
            if not toks_match_source(file_path, info["toks"]):
                print("You should not see this yet.")

        _parse_dump(file_path, parsed)
        remove(file_path + EXT_CTU_INFO)
        remove(file_path + EXT_DUMP)

    _dump_parsed(fname, parsed)
    return

def _parse_dump(analyzed_file_path: str, parsed: Dict[str, Dict[str, Dict[str, int|str|list ]]]) -> None:
    """
    Accepts the path to an analyzed file, as well as the parsed stdout
    of the run of CPPCheck.
    Navigates to the dump path of that file and adds information about
    which lines were tokenized to the dictionary.

    Parameters:
        analyzed_file_path (str): The path to an analyzed file.

    Returns:
        A dictionary of the following structure:
        {
            "path/to/file": {
                "preprocessor-config-name": {
                    "toks": "string-of-file-tokens",
                    "num_checks": number-of-checks-for-chunk (should be 27),
                    "errs": ["list", "of", "errors"],
                    "tokenized_lines": [tokenized, line, numbers]
                }, ... 
            }, ...
        }
    """
    dump_path: str = analyzed_file_path + EXT_DUMP
    if not path.exists(dump_path):
        return

    dump_xml: ET.ElementTree = ET.parse(dump_path)
    for dump in dump_xml.getroot().findall("dump"):
        config_name: str = dump.attrib["cfg"]
        tokens_raw: List[ET.Element] = dump.find("tokenlist").findall("token")
        tokens: Set[int] = {int(token.attrib["linenr"]) for token in tokens_raw if token.attrib["file"].split('.')[-1] != 'h'}
        parsed[analyzed_file_path][config_name]["tokenized_lines"] = list(tokens)

    return

# TODO
def toks_match_source(file_path: str, toks: str) -> bool:
    """
    *** STUB ***
    
    Accepts a file path and tokens for a preprocessor config of the file.
    Should open the provided file and determine if the tokens originate from this file.

    Parameters:
        file_path (str): The path to the file in question.
        toks (str): The tokens generated by the analyzer.

    Returns:
        True if we are confident that the tokens originate from this file.
        False otherwise.
    """
    return True

def _parse_stdout(stdout: str) -> Dict[str, Dict[str, Dict[str, int|str|list ]]]:
    """
    Accepts a '\n'-separated string and parses it into a usable dictionary.
    The string is intended to be the decoded stdout of a static analysis run.

    Parameters:
        stdout (str): The stdout of some static analysis.

    Returns:
        A dictionary of the following structure:
        {
            "path/to/file": {
                "preprocessor-config-name": {
                    "toks": "string-of-file-tokens",
                    "num_checks": number-of-checks-for-chunk (should be 27),
                    "errs": ["list", "of", "errors"]
                }, ... 
            }, ...
        }
    """
    parsed: Dict[str, Dict[str, Dict[str, int|str|list ]]] = {}
    stdout_split: List[str] = stdout.split('\n')

    is_next_line_toks: bool = False
    is_next_line_err: bool = False
    curr_fname: str = ""
    curr_config: str = ""

    for line in stdout_split:
        no_config: Optional[Match[str]] = match(PATTERN_FILE_NAME_NO_CONFIG, line)
        with_config: Optional[Match[str]] = match(PATTERN_FILE_NAME_WITH_CONFIG, line)

        # when the line is an error, combine in with the previous portion via a delimiter
        if is_next_line_err:
            parsed[curr_fname][curr_config]["errs"][-1] += DELIMITER + line

            # TODO: Scuffed-- maybe use a regex here instead
            is_next_line_err = '^' not in line

        # a filename is present but a preprocessor config is not
        elif no_config:
            split_fname: List[str] = line.split(' ')[1:-1]
            curr_fname = ' '.join(split_fname)
            curr_config = ""
            parsed[curr_fname] = {curr_config: {}}

        # a filename is present and a preprocessor config is
        # TODO: the re pattern may not handle all possible config names
        elif with_config:
            split_line: List[str] = line.split(':')
            curr_fname = split_line[0].split()[1]
            curr_config = split_line[1].strip().split('.')[0]

            if parsed.get(curr_fname) is None:
                parsed[curr_fname] = {}
            parsed[curr_fname][curr_config] = {}

        elif match(PATTERN_FILE_TOKENS, line):
            is_next_line_toks = True
            
        # depends on the previous elif
        elif is_next_line_toks:
            parsed[curr_fname][curr_config]["toks"] = line
            is_next_line_toks = False
            
        elif match(PATTERN_NUM_CHECKS, line):
            num_with_excess: str = line.split("Ran")[1]
            num_checks: int = int(num_with_excess.split("checks")[0])
            parsed[curr_fname][curr_config]["num_checks"] = num_checks

        elif match(PATTERN_ERROR, line):
            parsed[curr_fname][curr_config]["errs"] = [line]
            is_next_line_err = True

    # if a file has no errors, set "errs" to a placeholder value
    for path, configs in parsed.items():
        for config_name, info in configs.items():
            if "errs" not in info:
                info["errs"] = []

    return parsed

def _dump_parsed(fname: str, parsed: Dict[str, Dict[str, Dict[str, int|str|list ]]]) -> None:
    """
    Simple JSON write.

    Parameters:
        fname (str): The JSON file to write to.
        parsed (Dict[...]): The analysis performed by _parse_stdout.

    Returns:
        None.
    """
    with open(fname, 'w') as file:
        file.write(dumps(parsed, indent=2))

    return

