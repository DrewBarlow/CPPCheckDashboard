from json import dumps
from .loading import LoadingIcon
from re import Match, match
from subprocess import check_output, STDOUT
from typing import Dict, List, Optional

DELIMITER: str = "|%$%|"
NUM_ERROR_LINES: int = 2
PATTERN_FILE_PATH: str = r"(.*?\.[\w:]+)"
PATTERN_ERROR: str = fr"^{PATTERN_FILE_PATH}:\d+:\d+: error:"
PATTERN_FILE_NAME_NO_CONFIG: str = fr"^Checking {PATTERN_FILE_PATH} ...$"
PATTERN_FILE_NAME_WITH_CONFIG: str = r"^Checking {PATTERN_FILE_PATH}: \w+...$"
PATTERN_NUM_CONFIGS: str = r"^~~~ NUMBER OF PREPROCESSOR CONFIGS: \d+ ~~~$"
PATTERN_FILE_TOKENS: str = r"^~~~ FILE TOKENS: ~~~$"
PATTERN_NUM_CHECKS: str = r"^~~~ Ran \d+ checks for this above chunk ~~~$"

def analyze(path_to_analyzer: str, cmd_args: str, *, fname: str) -> None:
    """
    Accepts command line arguments to be passed to a static analyzer
    as if we were just running that analyzer.

    Parameters:
        path_to_analyzer (str): The analyzer in question.
        cmd_args (str): The arguments to be passed to the analyzer.
        fname (str): The resulting file to dump the parsed info to.

    Returns:
        None.
    """
    output_raw: Optional[bytes] = None
    stdout: str = ""
    parsed: Dict[str, Dict[str, Dict[str, int|str|List[str] ]]] = {}
    
    with LoadingIcon(f"Running {path_to_analyzer} {cmd_args}..."):
        output_raw = check_output(f"{path_to_analyzer} {cmd_args}", stderr=STDOUT, shell=True)

    with LoadingIcon("Parsing output..."):
        stdout = output_raw.decode()
        parsed = _parse_stdout(stdout)

    for i, (file_path, config_dict) in enumerate(parsed.items()):
        for config_name, info in config_dict.items():
            if info.get("toks") is None:
                print(file_path, config_name)
            if not toks_match_source(file_path, info["toks"]):
                print("You should not see this yet.")
            
    _dump_parsed(fname, parsed)
    return

# TODO
def toks_match_source(file_path: str, toks: str) -> bool:
    """
    *** STUB ***
    
    Accepts a file path and tokens for a preprocessor config of the file.
    Should open the provided file and determine if the tokens originate from this file.

    Parameters:
        file_path (str): The path to the file in question.
        toks (str): The tokens generated by the analyzer.

    Returns:
        True if we are confident that the tokens originate from this file.
        False otherwise.
    """
    return True

def _parse_stdout(stdout: str) -> Dict[str, Dict[str, Dict[str, int|str|List[str] ]]]:
    """
    Accepts a '\n'-separated string and parses it into a usable dictionary.
    The string is intended to be the decoded stdout of a static analysis run.

    Parameters:
        stdout (str): The stdout of some static analysis.

    Returns:
        A dictionary of the following structure:
        {
            "path/to/file": {
                "preprocessor-config-name": {
                    "toks": "string-of-file-tokens",
                    "num_checks": number-of-checks-for-chunk (should be 27),
                    "errs": ["list", "of", "errors"]
                }, ... 
            }, ...
        }
    """
    parsed: Dict[str, Dict[str, Dict[str, int|str|List[str] ]]] = {}
    stdout_split: List[str] = stdout.split('\n')

    is_next_line_toks: bool = False
    is_next_line_err: bool = False
    curr_fname: str = ""
    curr_config: str = ""

    for line in stdout_split:
        no_config: Optional[Match[str]] = match(PATTERN_FILE_NAME_NO_CONFIG, line)
        with_config: Optional[Match[str]] = match(PATTERN_FILE_NAME_WITH_CONFIG, line)

        # when the line is an error, combine in with the previous portion via a delimiter
        if is_next_line_err:
            parsed[curr_fname][curr_config]["errs"][-1] += DELIMITER + line

            # TODO: Scuffed-- maybe use a regex here instead
            is_next_line_err = '^' not in line

        # a filename is present but a preprocessor config is not
        elif no_config:
            split_fname: List[str] = line.split(' ')[1:-1]
            curr_fname = ' '.join(split_fname)
            curr_config = ""
            parsed[curr_fname] = {curr_config: {}}

        # a filename is present and a preprocessor config is
        # TODO: the re pattern may not handle all possible config names
        elif with_config:
            split_line: List[str] = line.split(':')
            curr_fname = split_line[0].split()[1]
            curr_config = split_line[1].strip().split('.')[0]

            if parsed.get(curr_fname) is None:
                parsed[curr_fname] = {}
            parsed[curr_fname][curr_config] = {}

        elif match(PATTERN_FILE_TOKENS, line):
            is_next_line_toks = True
            
        # depends on the previous elif
        elif is_next_line_toks:
            parsed[curr_fname][curr_config]["toks"] = line
            is_next_line_toks = False
            
        elif match(PATTERN_NUM_CHECKS, line):
            num_with_excess: str = line.split("Ran")[1]
            num_checks: int = int(num_with_excess.split("checks")[0])
            parsed[curr_fname][curr_config]["num_checks"] = num_checks

        elif match(PATTERN_ERROR, line):
            parsed[curr_fname][curr_config]["errs"] = [line]
            is_next_line_err = True

    # if a file has no errors, set "errs" to a placeholder value
    for path, configs in parsed.items():
        for config_name, info in configs.items():
            if "errs" not in info:
                info["errs"] = [""]

    return parsed

def _dump_parsed(fname: str, parsed: Dict[str, Dict[str, Dict[str, int|str|List[str] ]]]) -> None:
    """
    Simple JSON write.

    Parameters:
        fname (str): The JSON file to write to.
        parsed (Dict[...]): The analysis performed by _parse_stdout.

    Returns:
        None.
    """
    with open(fname, 'w') as file:
        file.write(dumps(parsed, indent=2))

    return

